{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the most difficult part of a project? Finding a great name for it. Is especially true for movies, because the need to be received well by the costumers. But how could you now the preferences of the average user? Easy! Use a AI trained on dataset by Netflix. This dataset is called Movielens 100k and contains the data, movie titles and 100k ratings by users of the targeted audience for movies\n",
    "\n",
    "Target of the project:\n",
    "-Train a neural network on the Movielens 100k dataset, to get a model that can predicted the rating of a given title for a new movie\n",
    "\n",
    "-Get a function that can help to improve titles.\n",
    "\n",
    "So lets get started. First of all, the needed Libarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "\n",
    "# this line must be executed once, then never again\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import the Dataset and all the words in the English language. We need the words so we can build new titles in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import titles and ratings \n",
    "title_df = pd.read_csv(\"https://files.grouplens.org/datasets/movielens/ml-100k/u.item\", sep='|', header=None, engine='python', encoding='latin-1')\n",
    "ratings_df = pd.read_csv(\"https://files.grouplens.org/datasets/movielens/ml-100k/u.data\",  delim_whitespace=True, header=None, engine='python', encoding='latin-1')\n",
    "\n",
    "# all words in the English language\n",
    "words_english = pd.read_json(\"https://raw.githubusercontent.com/dwyl/english-words/master/words_dictionary.json\",orient='index')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we extracted the titles from the data frame into a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the raw titles \n",
    "titles = title_df[1].values\n",
    "# Next the dates are slit from the titles, so we only have the raw titles without date\n",
    "titles = [strip_title[:-7] for strip_title in titles]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to get all the ratings for the titles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list for the ratings\n",
    "ratings = [0 for _ in range(0,len(titles))]\n",
    "\n",
    "# Get the average rating for each movie\n",
    "for idx,rating in enumerate(ratings_df[2]):\n",
    "    \n",
    "    # Not yet any rating for this index\n",
    "    if ratings[ratings_df[1][idx]-1] == 0:\n",
    "        ratings[ratings_df[1][idx]-1] = rating\n",
    "\n",
    "    # Add rating and avarage\n",
    "    else:\n",
    "        ratings[ratings_df[1][idx]-1] = np.average([ratings[ratings_df[1][idx]-1],rating])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the data we need in lists. But how do we import the data into a neural network, because a neural network can´t accepted text. For this Project there 3 approaches that were tried:\n",
    "\n",
    "1.Get all the words of all the titles and an use the list-index as an input\n",
    "Disadvantages:\n",
    "-For new titles only old words can be used without retraining the whole network\n",
    "-Syntax, semantic and relations between words are lost\n",
    "\n",
    "You can instead use a ‘Word-to-Vec’ model. This model takes a word as an input and outputs a vector. In this vector syntax, semantic and relations between words are preserved \n",
    "\n",
    "2.Use a Word-to-Vec model an train it on the titles\n",
    "Disadvantages:\n",
    "-There are only 1682 titles in the dataset. That’s not really enough data for this model\n",
    "\n",
    "3.Use a large pretrained model\n",
    "Disadvantages:\n",
    "-Model must be imported an can be some Gigabytes large\n",
    "-Some words are not in the model\n",
    "\n",
    "\n",
    "In the end option 3 was used, because it would yield the best performance. The large pretrained model used is googles 'GoogleNews-vectors-negative300'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word_to_vec_model = Word2Vec(vector_size=300, min_count=1, window=1,workers=2)\n",
    "\n",
    "# Combine our words in our title and all the words in the english vocabulary, so we can define new titels with words from the english language\n",
    "Vocabualary = np.array(titles + list(words_english.index))\n",
    "\n",
    "# The next lines insert the google model into our model. Code is from https://phdstatsphys.wordpress.com/2018/12/27/word2vec-how-to-train-and-update-it/\n",
    "Vocabualary = [nltk.word_tokenize(word) for word in Vocabualary]\n",
    "Word_to_vec_model.build_vocab(Vocabualary)\n",
    "Word_to_vec_model.wv.vectors_lockf = np.ones(len(Word_to_vec_model.wv), dtype=np.float32)\n",
    "Word_to_vec_model.wv.intersect_word2vec_format(r'C:\\Users\\alber\\OneDrive\\Dokumente\\Python Scripts\\MovieLens_100K_Dataset\\GoogleNews-vectors-negative300.bin.gz', lockf = 1, binary= True)\n",
    "\n",
    "# The model is train for one epoch to get the vectors for each word in titles, but compute_loss = False so the model is not changed\n",
    "Word_to_vec_model.train(Vocabualary, epochs=1, total_examples = 1, compute_loss = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next all the words in the titles are replaced into the according vectors from the Word-to-Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locks strange is strange. We need to define each title with the shape [20,300], due to this we need to fill the empty spots with something. In this case zeros. That’s not optimal, because 0 is a value, but NaN can’t be used as the input for the neural network later\n",
    "titles_with_vec = [[[np.float32(0)]*300,[np.float32(0)]*300,[np.float32(0)]*300,[np.float32(0)]*300,[np.float32(0)]*300,[np.float32(0)]*300,[np.float32(0)]*300,[np.float32(0)]*300,[np.float32(0)]*300,[np.float32(0)]*300,[np.float32(0)]*300,[np.float32(0)]*300,[np.float32(0)]*300,[np.float32(0)]*300,[np.float32(0)]*300,[np.float32(0)]*300,[np.float32(0)]*300,[np.float32(0)]*300,[np.float32(0)]*300,[np.float32(0)]*300] for _ in titles]\n",
    "\n",
    "titles = [nltk.word_tokenize(word) for word in titles]\n",
    "\n",
    "for idx_title,title in enumerate(titles):\n",
    "\n",
    "    for idx_word,word in enumerate(title):\n",
    "\n",
    "        titles_with_vec[idx_title][idx_word] = list(Word_to_vec_model.wv.get_vector(word, norm=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into train, validation and test set. Split is defined as a function so the ratio can be changed easily \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(ratio):\n",
    "\n",
    "    # Converted to tensor to get the data in the keras pipeline\n",
    "    x_train = tf.convert_to_tensor(titles_with_vec[:int(len(titles_with_vec)*ratio[0])])\n",
    "    y_train = tf.convert_to_tensor(ratings[:int(len(ratings)*ratio[0])])\n",
    "    x_val = tf.convert_to_tensor(titles_with_vec[int(len(titles_with_vec)*ratio[0]):int(len(titles_with_vec)*(ratio[0]+ratio[1]))])\n",
    "    y_val = tf.convert_to_tensor(ratings[int(len(ratings)*ratio[0]):int(len(ratings)*(ratio[0]+ratio[1]))])\n",
    "    x_test = tf.convert_to_tensor(titles_with_vec[int(len(titles_with_vec)*(ratio[0]+ratio[1])):])\n",
    "    y_test = tf.convert_to_tensor(ratings[int(len(ratings)*(ratio[0]+ratio[1])):])\n",
    "\n",
    "    return [x_train, y_train, x_val, y_val, x_test, y_test]\n",
    "\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = split([0.7,0.2,0.1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train the model and look at the loss, to get a clue what we need to optimizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.InputLayer(input_shape=(20,300)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(256, activation='relu'),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='relu')\n",
    "])\n",
    "\n",
    "title_model.summary()\n",
    "\n",
    "loss_fn = tf.keras.losses.MeanAbsoluteError()\n",
    "sampleID = 100\n",
    "loss_fn(y_train[sampleID].numpy(), title_model(x_train[sampleID-1:sampleID]).numpy()).numpy()\n",
    "\n",
    "title_model.compile(optimizer='Adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['mean_absolute_error'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train the model and look at the loss, to get a clue what we need to optimies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=2,\n",
    "    validation_data=(x_val, y_val),\n",
    ")\n",
    "\n",
    "plt.plot(title_model.history.history['loss'])\n",
    "plt.plot(title_model.history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output we can see that we have significate Problems with overfitting. To reduce overfitting in the new model l2 is used and the learning rate is reduced. The learning rate was determined by trial and error. Also an early stopping was added so we don´t get into overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.InputLayer(input_shape=(20,300)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(20, activation=tf.nn.relu, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "  tf.keras.layers.Dense(20, activation=tf.nn.relu, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='relu')\n",
    "])\n",
    "\n",
    "title_model.summary()\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.00001)\n",
    "loss_fn = tf.keras.losses.MeanAbsoluteError()\n",
    "sampleID = 100\n",
    "loss_fn(y_train[sampleID].numpy(), title_model(x_train[sampleID-1:sampleID]).numpy()).numpy()\n",
    "\n",
    "title_model.compile(optimizer=optimizer,\n",
    "                    loss=loss_fn,\n",
    "                    metrics=['mean_absolute_error'])\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                      min_delta=0,\n",
    "                                      patience=1,\n",
    "                                      mode='auto'\n",
    "                                      )\n",
    "title_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    batch_size=2,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=[es]\n",
    ")\n",
    "\n",
    "plt.plot(title_model.history.history['loss'])\n",
    "plt.plot(title_model.history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "_, validation_acc = title_model.evaluate(x_test, y_test)\n",
    "print('Testset absulut error:', validation_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overfitting problem is now better until around epoch 95, but at this point the performance didn´t really improve with changes to the network. Maybe this is due to the small size of the dataset, with only 1686 titles. With this dataset you get in to overfitting really fast and the complexity of the network must be small.\n",
    "To finish the Project we need a function to evaluate a new title:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_a_title(title):\n",
    "\n",
    "    # Split the words and reform title\n",
    "    title = title.split()\n",
    "    title = [nltk.word_tokenize(word) for word in title]\n",
    "    title_in_vec = []\n",
    "\n",
    "    # Create the correct input for the network \n",
    "    for x in range(0,20):\n",
    "\n",
    "        if x < len(title):\n",
    "            title_in_vec.append(list(Word_to_vec_model.wv.get_vector(title[x][0], norm=True)))\n",
    "        \n",
    "        else:\n",
    "            title_in_vec.append([np.float32(0)]*300)\n",
    "    \n",
    "    # Return the rating\n",
    "    return title_model( tf.convert_to_tensor([title_in_vec])).numpy()[0][0]\n",
    "\n",
    "\n",
    "# Try out the title of our new movie \n",
    "print(f\"Your title is going to rate: {evaluate_a_title('hello world')} with a toleranze of {str(validation_acc)}, max. rsting is 5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future improvements:\n",
    "- Train the model on for example the movielens 25m Dataset\n",
    "- Add the Word2Vec as embedding layer so you are not restricted to the English language \n",
    "\n",
    "Conclusion:\n",
    "\n",
    "The evaluate_a_title() function is as it is not a tool with can be used without understanding the limitations, but you can use it to give you some directions for Netflix movie titles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7c39e21a772e99c1a7e3fb7ef3402580462d255e64df51b314942f2f47217269"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
